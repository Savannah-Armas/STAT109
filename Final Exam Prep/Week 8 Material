------------------------------------------------------------------------------------------
          LECTURE
------------------------------------------------------------------------------------------
#Week-8
#Video: https://youtu.be/ICi8MqvE_40
library(DAAG)
library(MASS)
library(compositions)
library(psych)
library(car)
library(PerformanceAnalytics)
library(olsrr)

# 
str(Salaries)  
model.matrix(~rank, Salaries)
m <- lm(salary ~ ., Salaries)
summary(m)
# The below model includes only those significant vairables 
m2 <- lm(salary ~ . - sex, Salaries)
summary(m2)

# Partition
set.seed(25)
ind <- sample(2, nrow(Salaries), replace = T, prob = c(0.5, 0.5))
trg <- Salaries[ind == 1,]
test <- Salaries[ind == 2,]

m <- lm(salary ~ ., trg)
summary(m)

# Trying all possible variable combinations - 31 models
(all <- ols_step_all_possible(m)) 

model <- stepAIC(m, direction = "both", trace = F)
summary(model)

p <- predict(model, test)
plot(test$salary, p)
c <- cor(p, test$salary)
(R2 <- c^2)

# Interaction
m <- lm(salary ~ .*., Salaries) # .*. includes all interaction vars. 
m <- lm(salary ~ . + disciplineB:yrs.service, Salaries) # includes selected interaction vars. 

# In Console: head(p)
# In Console: head(test$salary)

#Polynomials
m <- lm()

# 
str(allbacks)
data <- allbacks[-4]
data$X3 <- rnorm(15)
data$X4 <- rnorm(15, 6, 2)

full <- lm(weight ~ ., data)
red <- lm(weight ~ volume + area, data)
anova(red, full)

# 
Y <- c(43.6, 38, 30.1, 35.3, 46.4, 34.2, 30.2, 40.7, 38.5, 22.6, 37.6)
X1 <- c(12, 11, 9, 7, 12, 8, 6, 13, 8, 6, 8)
X2 <- c(13.9, 12, 9.3, 9.7, 12.3, 11.4, 9.3, 14.3, 10.2, 8.4, 11.2)
pizza <- data.frame(X1, X2, Y)
m1 <- lm(Y ~ X1, pizza)
summary(m1)

# 
data(Coxite)
str(Coxite)
?Coxite
data <- data.frame(Coxite)
str(data)
pairs.panels(data, gap = 0)
cor.plot(data)
chart.Correlation(data)

# 
m <- lm(porosity ~ . , data)
summary(m)

m <- lm(porosity ~ . -E, data) #Removing highly correlated var
summary(m)
vif(m)
AIC(m)

m <- lm(porosity ~ . -E - D -A, data) #Removing additional highly correlated vars
summary(m)
vif(m)
AIC(m)

# Read a CSV File
data <- read.csv()

------------------------------------------------------------------------------------------
          SECTION
------------------------------------------------------------------------------------------

---
title: "Section 8: Multilinear Regression Strategies & Multicollinearity"
author: "Dikhit, Anshuman"
date: "3/18/2024"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen=5)
```

Import necessary libraries
```{r}
library(MASS)
```

# Section 8

Goal of this section is to cover the following:

- Multilinear Regression model
    - Model building & Interpretation
    - Model tuning
        - Stepwise AIC
        - Multicollinearity

### Data Description & Setup

We will use the provided `insuyrance.csv` dataset for this section. This dataset provides the following data on 1338 medically-insured individuals:

Predictors:
- `age`: Age of Primary Beneficiary
- `sex`: Gender of individual (male or female)
- `bmi`: Body Mass Index of individual
- `children`: Number of children covered by individuals health insurance
- `smoker`: Whether individual smokes or not
- `region`: Residential area in the US of beneficiary (northeast, southeast, southwest, northwest)

Y variable:
- `charges`: Individual medical costs billed by health insurance


Run the provided code below, this will import the dataset and perform some preliminary data cleaning to get our dataframe ready for modeling.
```{r}
insurance.df <- read.csv("insurance.csv")
factor.cols <- c("region", "sex", "smoker")
insurance.df[factor.cols] <- lapply(insurance.df[factor.cols], factor)
head(insurance.df)
```

### Model building

a) Start by generating the full multilinear regression model for our dataset. 
```{r}
base.model <- lm(charges ~ ., data = insurance.df)
summary(base.model)
```


b) Interpret the value of the coefficient for `age`. What does that value represent with respect to the regression model?

c) Identify all statistically insignificant predictors in the full model. What is the null hypothesis for this process?

d) Let's see what happens when we remove these predictors out of our model. Which predictors must we remove? What is our new Adjusted R-squared value?

This "iterative" process we just undertook is stepwise regression. We can do this process manually ourselves, or we can just have R automate it for us via the `stepAIC()` function:

e) Use the stepAIC function to get the optimal model.
```{r}
optimal.model <- stepAIC(base.model, direction="both")
summary(optimal.model)
```

f) Check for multicollinearity within the optimal model.
```{r}
vif(optimal.model)
```

g) Using the optimal model, calculate the medical expenses for the following "individuals":

- A 45-year old non-smoking male with 2 children, a BMI of 29.4, & in the southwest region:
```{r}
predict(
    optimal.model, 
    newdata=data.frame(
        age=45,
        sex="male",
        smoker="no",
        children=2,
        bmi=29.4,
        region="southwest"
    )
)
```
- A 45 year old smoking male with 2 children, a BMI of 29.4, & in the southeast region:
```{r}
predict(
    optimal.model, 
    newdata=data.frame(
        age=45,
        sex="male",
        smoker="yes",
        children=2,
        bmi=29.4,
        region="southeast"
    )
)
```

- A 27 year old non-smoking female with no children, a BMI of 24.4, & in the northwest region:
```{r}
predict(
    optimal.model, 
    newdata=data.frame(
        age=27,
        sex="female",
        smoker="no",
        children=0,
        bmi=24.4,
        region="northwest"
    )
)
```
- A 27 year old smoking female with no children, a BMI of 24.4, & in the northwest region:
```{r}
predict(
    optimal.model, 
    newdata=data.frame(
        age=27,
        sex="female",
        smoker="yes",
        children=0,
        bmi=24.4,
        region="northwest"
    )
)
