------------------------------------------------------------------------------------------
          LECTURE
------------------------------------------------------------------------------------------
#Week-8
#Video: https://youtu.be/ICi8MqvE_40
library(DAAG)
library(MASS)
library(compositions)
library(psych)
library(car)
library(PerformanceAnalytics)
library(olsrr)

# 
str(Salaries)  
model.matrix(~rank, Salaries)
m <- lm(salary ~ ., Salaries)
summary(m)
# The below model includes only those significant vairables 
m2 <- lm(salary ~ . - sex, Salaries)
summary(m2)

# Partition
set.seed(25)
ind <- sample(2, nrow(Salaries), replace = T, prob = c(0.5, 0.5))
trg <- Salaries[ind == 1,]
test <- Salaries[ind == 2,]

m <- lm(salary ~ ., trg)
summary(m)

#
(all <- ols_step_all_possible(m)) 

model <- stepAIC(m, direction = "both", trace = F)
summary(model)

p <- predict(model, test)
plot(test$salary, p)
c <- cor(p, test$salary)
(R2 <- c^2)

# Interaction
m <- lm(salary ~ .*., Salaries)
m <- lm(salary ~ . + , Salaries)

# In Console: head(p)
# In Console: head(test$salary)

#Polynomials
m <- lm()

# 
str(allbacks)
data <- allbacks[-4]
data$X3 <- rnorm(15)
data$X4 <- rnorm(15, 6, 2)

full <- lm(weight ~ ., data)
red <- lm(weight ~ volume + area, data)
anova(red, full)

# 
Y <- c(43.6, 38, 30.1, 35.3, 46.4, 34.2, 30.2, 40.7, 38.5, 22.6, 37.6)
X1 <- c(12, 11, 9, 7, 12, 8, 6, 13, 8, 6, 8)
X2 <- c(13.9, 12, 9.3, 9.7, 12.3, 11.4, 9.3, 14.3, 10.2, 8.4, 11.2)
pizza <- data.frame(X1, X2, Y)
m1 <- lm(Y ~ X1, pizza)
summary(m1)

# 
data(Coxite)
str(Coxite)
?Coxite
data <- data.frame(Coxite)
str(data)
pairs.panels(data, gap = 0)
cor.plot(data)
chart.Correlation(data)

# 
m <- lm(porosity ~ . , data)
summary(m)

m <- lm(porosity ~ . -E, data)
summary(m)
vif(m)
AIC(m)

m <- lm(porosity ~ . -E - D -A, data)
summary(m)
vif(m)
AIC(m)

# Read a CSV File
data <- read.csv()

------------------------------------------------------------------------------------------
          SECTION
------------------------------------------------------------------------------------------

---
title: "Section 8: Multilinear Regression Strategies & Multicollinearity"
author: "Dikhit, Anshuman"
date: "3/18/2024"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen=5)
```

Import necessary libraries
```{r}
library(MASS)
```

# Section 8

Goal of this section is to cover the following:

- Multilinear Regression model
    - Model building & Interpretation
    - Model tuning
        - Stepwise AIC
        - Multicollinearity

### Data Description & Setup

We will use the provided `insuyrance.csv` dataset for this section. This dataset provides the following data on 1338 medically-insured individuals:

Predictors:
- `age`: Age of Primary Beneficiary
- `sex`: Gender of individual (male or female)
- `bmi`: Body Mass Index of individual
- `children`: Number of children covered by individuals health insurance
- `smoker`: Whether individual smokes or not
- `region`: Residential area in the US of beneficiary (northeast, southeast, southwest, northwest)

Y variable:
- `charges`: Individual medical costs billed by health insurance


Run the provided code below, this will import the dataset and perform some preliminary data cleaning to get our dataframe ready for modeling.
```{r}
insurance.df <- read.csv("insurance.csv")
factor.cols <- c("region", "sex", "smoker")
insurance.df[factor.cols] <- lapply(insurance.df[factor.cols], factor)
head(insurance.df)
```

### Model building

a) Start by generating the full multilinear regression model for our dataset. 
```{r}
base.model <- lm(charges ~ ., data = insurance.df)
summary(base.model)
```


b) Interpret the value of the coefficient for `age`. What does that value represent with respect to the regression model?

c) Identify all statistically insignificant predictors in the full model. What is the null hypothesis for this process?

d) Let's see what happens when we remove these predictors out of our model. Which predictors must we remove? What is our new Adjusted R-squared value?

This "iterative" process we just undertook is stepwise regression. We can do this process manually ourselves, or we can just have R automate it for us via the `stepAIC()` function:

e) Use the stepAIC function to get the optimal model.
```{r}
optimal.model <- stepAIC(base.model, direction="both")
summary(optimal.model)
```

f) Check for multicollinearity within the optimal model.
```{r}
vif(optimal.model)
```

g) Using the optimal model, calculate the medical expenses for the following "individuals":

- A 45-year old non-smoking male with 2 children, a BMI of 29.4, & in the southwest region:
```{r}
predict(
    optimal.model, 
    newdata=data.frame(
        age=45,
        sex="male",
        smoker="no",
        children=2,
        bmi=29.4,
        region="southwest"
    )
)
```
- A 45 year old smoking male with 2 children, a BMI of 29.4, & in the southeast region:
```{r}
predict(
    optimal.model, 
    newdata=data.frame(
        age=45,
        sex="male",
        smoker="yes",
        children=2,
        bmi=29.4,
        region="southeast"
    )
)
```

- A 27 year old non-smoking female with no children, a BMI of 24.4, & in the northwest region:
```{r}
predict(
    optimal.model, 
    newdata=data.frame(
        age=27,
        sex="female",
        smoker="no",
        children=0,
        bmi=24.4,
        region="northwest"
    )
)
```
- A 27 year old smoking female with no children, a BMI of 24.4, & in the northwest region:
```{r}
predict(
    optimal.model, 
    newdata=data.frame(
        age=27,
        sex="female",
        smoker="yes",
        children=0,
        bmi=24.4,
        region="northwest"
    )
)
```
---
title: "Practice Quiz Answers"
output: pdf_document
date: '2022-04-22'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(ISLR)
library(rpart)
library(rpart.plot)
library(mlbench)
library(caret)
library(MASS)
library(DAAG)
library(ISLR)
library(foreign)
library(e1071)
library(pROC)
library(lattice)
library(lime)
library(dplyr)
```


## Practice Quiz Answers 

# Question 1: 

Which of the following methods could be applied to detect a binary choice? 
A. Logistic Regression
C. Classification Tree
D. Random Forest 

# Question 2: 

For which type of models are ROC curves appropriate? 
C. Classification Tree - ROC curves are appropriate for models that have a binary dependent variable. 

# Question 3: 

In a classification tree, where is the most important independent variable located? 
B. Root node 

# Question 4: 

What is the final prediction in a classification tree model based on? 
B. Majority

# Question 5: 

What is the final prediction in a regression tree model based on? 
A. Average

# Question 6: 
In which of the following applications can a tree-based algorithm be successfully applied? 
A. Predicting if a football team will win, lose, or tie based on previous data 
B. Predict the sales value of a clothing company based on previous sales 
C. Predict the weather based on historical data 

# Question 7: 

The numerous trees/models in a random forest are always dependent on each other. 
False - The models in random forest and bagging are independent of each other. The trees in extreme gradient boosting are dependent of each other. 

# Question 8: 

All ensemble methods (bagging, random forest, and boosting) can be used for classification and regression problems. 
True 

# Question 9: 

The smaller the complexity parameter value, the smaller the tree. 
False 

# Question 10: 

An individual decision tree model has less variability than a random forest model. 
False 

# Question 11:

When you use bootstrap sampling, you will never choose the same data point twice. 
False - bootstrap sampling is with replacement, so you may choose the same data point multiple times! 

# Question 12: 

How do you use the accuracy of the testing/training data to determine a best model? And how do you recognize when over-fitting is happening using accuracy? 

You look for the highest accuracy for the TESTING data. If the model performs significantly better on the training set than the testing set, we can conclude that over-fitting is present. 

# Question 13: 

How is random forest similar/different from bagging? 

Both Random forest and Bagging methods use bootstrap sampling to create their models; however, random forest chooses a random sample from all the independent variables to use for each tree, while bagging uses all of the independent variables for each tree. 

# Question 14: 

How do we consider xerror when evaluating the performance of a model? What is the relationship between xerror and complexity parameter? 

Well performing models have a low xerror value. Typically, as you decrease the complexity parameter, the xerror also decreases; however, after a certain point the xerror will being to increase again. 

# Question 15: 

It is a classification tree, and we can predict the new passenger survived. This subset of passengers (women who were part of the upper class) represents 22% of all observations in this dataset. Additionally, if you were a woman and part of upper class, you had an overall survival probability of 94%. 

```{r}
set.seed(1234)
ind <- sample(2, nrow(titanic), replace = T, prob = c(0.8, 0.2))

train <- titanic[ind == 1,]
test <- titanic[ind == 2,]

tree <- rpart(Survived ~., data = titanic)
rpart.plot(tree)
printcp(tree)
plotcp(tree)
```


# Question 16: 

Using the 'Credit' dataset in the 'ISLR' library, develop a random forest classification model for 'Student' as a response. What are the top 3 most important variables? Note: Use set.seed(1234) and entire data for developing the model.

Balance, Rating, and Limit 

```{r}
?Credit
```


```{r}
data('Credit')
mydata <- Credit

ind <- sample(2, nrow(mydata), replace = T, prob = c(0.5, 0.5))
train <- mydata[ind == 1,]
test <- mydata[ind == 2,]
```

```{r}
student <- train(Student ~ Balance + Income + Limit + Education + Rating, data = train, method = "rf", importance = TRUE)
student
```

```{r}
plot(varImp(student))
```




